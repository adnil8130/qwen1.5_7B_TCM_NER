{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c4443b2-24ef-4fe4-b6e3-a88fb15e675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/my-env/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import jieba \n",
    "from rouge_chinese import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "\n",
    "qwen_model_path = \"/root/autodl-tmp/models--Qwen--Qwen1.5-7B-Chat/snapshots/5f4f5e69ac7f1d508f8369e977de208b4803444b\"\n",
    "lora_model_path = './tcm_ner/checkpoint-329/'\n",
    "data_files = '/root/sft_qwen1p5_7B/datasets/'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f46e66c-6302-42ea-baa7-0ec90a35dc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_dataset:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'entities'],\n",
      "        num_rows: 5259\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'entities'],\n",
      "        num_rows: 657\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'entities'],\n",
      "        num_rows: 658\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "raw_dataset = load_dataset('json', data_files=data_files+'/medical.train.json')\n",
    "raw_dataset_dev = load_dataset('json', data_files=data_files+'/medical.dev.json')\n",
    "raw_dataset_test = load_dataset('json', data_files=data_files+'/medical.test.json')\n",
    "raw_dataset['validation'] = raw_dataset_dev['train']\n",
    "raw_dataset['test'] = raw_dataset_test['train']\n",
    "columns = raw_dataset['train'].column_names\n",
    "print(\"raw_dataset: \", raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48efa924-5035-44aa-8b99-812ae52875bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c0f4c103a04724929b31069bbf26af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 基础模型加载\n",
    "tokenizer = AutoTokenizer.from_pretrained(qwen_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(qwen_model_path).half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a064f7b6-8cf8-4a1c-bbdf-e8148234370e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65632c834424daea0d6bdc2b513f092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/658 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  [151644, 8948, 271, 262, 220, 56568, 101909, 108704, 101565, 102450, 104799, 101057, 3837, 112735, 45181, 89012, 22382, 9370, 109949, 15946, 107439, 198, 262, 481, 72858, 115065, 46448, 198, 262, 481, 72858, 99286, 101899, 198, 262, 481, 72858, 99286, 33477, 99383, 198, 262, 481, 72858, 99286, 105262, 198, 262, 481, 72858, 99471, 198, 262, 481, 220, 104595, 101107, 198, 262, 481, 34369, 114, 42411, 101899, 198, 262, 481, 71773, 100067, 198, 262, 481, 8908, 58098, 99286, 101899, 198, 262, 481, 8908, 58098, 99286, 105262, 198, 262, 32181, 247, 97084, 101565, 13, 220, 23031, 2951, 51461, 120, 28330, 66017, 11, 69372, 5212, 2996, 4326, 788, 330, 100721, 73743, 497, 330, 2996, 6106, 788, 330, 102870, 9207, 1154, 5212, 2996, 4326, 788, 330, 99789, 120470, 497, 330, 2996, 6106, 788, 330, 104823, 105262, 9207, 715, 262, 97161, 25, 715, 262, 220, 16, 13, 70568, 9370, 104588, 22243, 71268, 100645, 20412, 105045, 2951, 73312, 38304, 51575, 13, 715, 262, 220, 17, 13, 46750, 122, 99828, 99885, 101565, 13343, 11, 70568, 1, 80443, 101958, 99885, 101565, 3263, 36845, 257, 151645, 198, 151644, 872, 198, 99471, 41299, 20109, 26022, 102843, 3837, 99789, 120470, 93266, 55806, 3837, 104579, 93266, 49185, 3837, 100150, 93266, 100096, 151645, 198, 151644, 77091, 198, 4913, 2996, 4326, 788, 330, 99789, 120470, 497, 330, 2996, 6106, 788, 330, 104823, 105262, 9207, 151643]\n",
      "inputs:  <|im_start|>system\n",
      "\n",
      "    你是一个文本实体识别领域的专家，你需要从给定的句子中提取\n",
      "    - 中医治则\n",
      "    - 中医治疗\n",
      "    - 中医证候\n",
      "    - 中医诊断\n",
      "    - 中药\n",
      "    - 临床表现\n",
      "    - 其他治疗\n",
      "    - 方剂\n",
      "    - 西医治疗\n",
      "    - 西医诊断\n",
      "    这些实体. 以 json 格式输出, 如 {\"entity_text\": \"丹参\", \"entity_label\": \"中药\"} , {\"entity_text\": \"黄疸\", \"entity_label\": \"中医诊断\"} \n",
      "    注意: \n",
      "    1. 输出的每一行都必须是正确的 json 字符串. \n",
      "    2. 找不到任何实体时, 输出\"没有找到任何实体\". \n",
      "    \n",
      "    <|im_end|>\n",
      "<|im_start|>user\n",
      "药进１０帖，黄疸稍退，饮食稍增，精神稍振<|im_end|>\n",
      "<|im_start|>assistant\n",
      "{\"entity_text\": \"黄疸\", \"entity_label\": \"中医诊断\"}<|endoftext|>\n",
      "label_ids:  [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4913, 2996, 4326, 788, 330, 99789, 120470, 497, 330, 2996, 6106, 788, 330, 104823, 105262, 9207, 151643]\n",
      "labels:  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------{\"entity_text\": \"黄疸\", \"entity_label\": \"中医诊断\"}<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "def tranfer_old_to_new(example):\n",
    "    \"\"\"\n",
    "    用数据集拼system prompt和输入输出\n",
    "    example:\n",
    "    {'text': '目的观察复方丁香开胃贴外敷神阙穴治疗慢性心功能不全伴功能性消化不良的临床疗效', 'entities': [{'end_idx': 10, 'entity_label': '中医治疗', 'entity_text': '复方丁香开胃贴', 'start_idx': 4}, {'end_idx': 32, 'entity_label': '西医诊断', 'entity_text': '心功能不全伴功能性消化不良', 'start_idx': 20}]}\n",
    "    \n",
    "    res:\n",
    "    {'instruction': '\\n    你是一个文本实体识别领域的专家，你需要从给定的句子中提取\\n    - 中医治则\\n    - 中医治疗\\n    - 中医证候\\n    - 中医诊断\\n    - 中药\\n    - 临床表现\\n    - 其他治疗\\n    - 方剂\\n    - 西医治疗\\n    - 西医诊断\\n    这些实体. 以 json 格式输出, 如 {\"entity_text\": \"丹参\", \"entity_label\": \"中药\"} , {\"entity_text\": \"黄疸\", \"entity_label\": \"中医诊断\"} \\n    注意: \\n    1. 输出的每一行都必须是正确的 json 字符串. \\n    2. 找不到任何实体时, 输出\"没有找到任何实体\". \\n    \\n    ', 'input': '现头昏口苦', 'output': '{\"entity_text\": \"口苦\", \"entity_label\": \"临床表现\"}'}\n",
    "    \"\"\"\n",
    "    input_text = example[\"text\"]\n",
    "    entities = example[\"entities\"]\n",
    "        \n",
    "    entity_sentence = \"\"\n",
    "    for entity in entities:\n",
    "        entity_json = dict(entity)\n",
    "        entity_text = entity_json[\"entity_text\"]\n",
    "        entity_label = entity_json[\"entity_label\"]\n",
    "\n",
    "        entity_sentence += f\"\"\"{{\"entity_text\": \"{entity_text}\", \"entity_label\": \"{entity_label}\"}}\"\"\"\n",
    "\n",
    "    if entity_sentence == \"\":\n",
    "        entity_sentence = \"没有找到任何实体\"\n",
    "    \n",
    "    res = dict()\n",
    "    res[\"instruction\"] = \"\"\"\n",
    "    你是一个文本实体识别领域的专家，你需要从给定的句子中提取\n",
    "    - 中医治则\n",
    "    - 中医治疗\n",
    "    - 中医证候\n",
    "    - 中医诊断\n",
    "    - 中药\n",
    "    - 临床表现\n",
    "    - 其他治疗\n",
    "    - 方剂\n",
    "    - 西医治疗\n",
    "    - 西医诊断\n",
    "    这些实体. 以 json 格式输出, 如 {\"entity_text\": \"丹参\", \"entity_label\": \"中药\"} , {\"entity_text\": \"黄疸\", \"entity_label\": \"中医诊断\"} \n",
    "    注意: \n",
    "    1. 输出的每一行都必须是正确的 json 字符串. \n",
    "    2. 找不到任何实体时, 输出\"没有找到任何实体\". \n",
    "    \n",
    "    \"\"\"\n",
    "    res[\"input\"] = f\"{input_text}\"\n",
    "    res[\"output\"] = entity_sentence\n",
    "    return res\n",
    "\n",
    "def process_func(old_example):\n",
    "    example = tranfer_old_to_new(old_example)\n",
    "    MAX_LENGTH = 784\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(        \n",
    "        f\"<|im_start|>system\\n{example['instruction']}<|im_end|>\\n<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = (\n",
    "        instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    )\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        print(\"trunct!\", len(input_ids))\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "def print_dataset_example(example):\n",
    "    print(\"input_ids: \",example[\"input_ids\"])\n",
    "    print(\"inputs: \", tokenizer.decode(example[\"input_ids\"]))\n",
    "    print(\"label_ids: \", example[\"labels\"])\n",
    "    print(\"labels: \", tokenizer.decode(list(map(lambda x: x if x != -100 else 12, example[\"labels\"]))))\n",
    "test_dataset = raw_dataset['test'].map(\n",
    "                process_func,\n",
    "                load_from_cache_file=False,\n",
    "                desc=\"Running tokenizer on train dataset\",\n",
    "            )\n",
    "print_dataset_example(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67d745b1-688f-43f4-b826-33f24fc57caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_proprocess(example, tokenizer):\n",
    "    instruction = \"\"\"\n",
    "    你是一个文本实体识别领域的专家，你需要从给定的句子中提取\n",
    "    - 中医治则\n",
    "    - 中医治疗\n",
    "    - 中医证候\n",
    "    - 中医诊断\n",
    "    - 中药\n",
    "    - 临床表现\n",
    "    - 其他治疗\n",
    "    - 方剂\n",
    "    - 西医治疗\n",
    "    - 西医诊断\n",
    "    这些实体. 以 json 格式输出, 如 {\"entity_text\": \"丹参\", \"entity_label\": \"中药\"} , {\"entity_text\": \"黄疸\", \"entity_label\": \"中医诊断\"} \n",
    "    注意: \n",
    "    1. 输出的每一行都必须是正确的 json 字符串. \n",
    "    2. 找不到任何实体时, 输出\"没有找到任何实体\". \n",
    "    \n",
    "    \"\"\"\n",
    "    input_value = example['text']\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{instruction}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{input_value}\"}\n",
    "    ]\n",
    "    return input_value, messages\n",
    "\n",
    "def predict(messages, model, tokenizer):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response\n",
    "\n",
    "def compute_metrics(test_dataset, model, tokenizer, max_len=10000):\n",
    "    decoded_preds, decoded_labels = [], []\n",
    "    max_len = min(max_len, len(test_dataset))\n",
    "    for i in range(max_len):\n",
    "        print(\"test index: \", i)\n",
    "        input_value, messages = messages_proprocess(test_dataset[i], tokenizer)\n",
    "        response = predict(messages, model, tokenizer)\n",
    "        ground_truth = tokenizer.decode(list(filter(lambda x: x != -100, test_dataset[i][\"labels\"])))\n",
    "        decoded_preds.append(response)\n",
    "        decoded_labels.append(ground_truth)\n",
    "    \n",
    "    score_dict = {\n",
    "        \"rouge-1\": [],\n",
    "        \"rouge-2\": [],\n",
    "        \"rouge-l\": [],\n",
    "        \"bleu-4\": []\n",
    "    }\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        hypothesis = list(jieba.cut(pred))\n",
    "        reference = list(jieba.cut(label))\n",
    "        rouge = Rouge()\n",
    "        hypothesis = ' '.join(hypothesis)\n",
    "        if not hypothesis:\n",
    "            hypothesis = \"-\"\n",
    "        scores = rouge.get_scores(hypothesis, ' '.join(reference))\n",
    "        result = scores[0]\n",
    "    \n",
    "        for k, v in result.items():\n",
    "            score_dict[k].append(round(v[\"f\"] * 100, 4))\n",
    "        bleu_score = sentence_bleu([list(label)], list(pred), smoothing_function=SmoothingFunction().method3)\n",
    "        score_dict[\"bleu-4\"].append(round(bleu_score * 100, 4))\n",
    "    \n",
    "    for k, v in score_dict.items():\n",
    "        score_dict[k] = float(np.mean(v))\n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960c878-9d55-42bd-a7f5-cb4592a9ad85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "997bcdf3-f2eb-4563-8cd4-908f3a575d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test index:  0\n",
      "test index:  1\n",
      "test index:  2\n",
      "test index:  3\n",
      "test index:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.814 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score before train:  {'rouge-1': 57.33255999999999, 'rouge-2': 51.25812, 'rouge-l': 40.72986, 'bleu-4': 27.56572}\n"
     ]
    }
   ],
   "source": [
    "score_dict_before = compute_metrics(test_dataset, model, tokenizer, 5)\n",
    "print(\"score before train: \", score_dict_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a93004d4-db9e-4c80-9aef-3d094cb7d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora模型加载\n",
    "lora_model = PeftModel.from_pretrained(model, model_id=lora_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8c89538-2eb5-4182-9692-be36dd6be3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test index:  0\n",
      "test index:  1\n",
      "test index:  2\n",
      "test index:  3\n",
      "test index:  4\n",
      "score after train:  {'rouge-1': 75.28135999999999, 'rouge-2': 75.09954, 'rouge-l': 71.59864, 'bleu-4': 49.85774}\n"
     ]
    }
   ],
   "source": [
    "score_dict_after = compute_metrics(test_dataset, lora_model, tokenizer, 5)\n",
    "print(\"score after train: \", score_dict_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bafc3b-1b5d-49aa-8676-ee08fea5816b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env-kernel",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
